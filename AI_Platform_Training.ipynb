{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_Platform_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/tBsHrvs2JdphSNa4YD2g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKdM7dacl47"
      },
      "source": [
        "# Authentications\n",
        "First, let's authenticate the colab notebook to access both GCP and GEE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeT6PadnlsPe"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNkvzYnltZT"
      },
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTpbPty_doo0"
      },
      "source": [
        "# Imports\n",
        "Note that we downgraded tensorflow to `2.1.0`. That's because that's the tensorflow runtime version  that we use to train, eeify, and deploy the model later on. So we want to make sure that this colab notebook is using the same version.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOTS1O9S_gcw"
      },
      "source": [
        "!pip install tensorflow==2.1.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8lkli0O_4Qc"
      },
      "source": [
        "import folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiIiUK-znStk"
      },
      "source": [
        "# Training Job Preparation\n",
        "In this section, we'll create a training task with configurations and model definition file.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4AtG54N4T3m"
      },
      "source": [
        "PACKAGE_PATH = 'cdl_ai_platform'\n",
        "\n",
        "!ls -l\n",
        "!mkdir {PACKAGE_PATH}\n",
        "!touch {PACKAGE_PATH}/__init__.py\n",
        "!ls -l {PACKAGE_PATH}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLVNseHj4Y7o"
      },
      "source": [
        "%%writefile {PACKAGE_PATH}/config.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# INSERT YOUR PROJECT HERE!\n",
        "PROJECT = 'GCP project Id'\n",
        "\n",
        "# INSERT YOUR BUCKET HERE!\n",
        "BUCKET = 'your bucket'\n",
        "\n",
        "# Specify names of output locations in Cloud Storage.\n",
        "JOB_FOLDER = 'your folder'\n",
        "JOB_DIR = 'gs://' + BUCKET + '/' + JOB_FOLDER + '/trainer'\n",
        "MODEL_DIR = JOB_DIR + '/model'\n",
        "LOGS_DIR = JOB_DIR + '/logs'\n",
        "\n",
        "# Put the EEified model next to the trained model directory.\n",
        "EEIFIED_DIR = JOB_DIR + '/eeified'\n",
        "\n",
        "# Pre-computed training and eval data.\n",
        "DATA_BUCKET = 'bucket from data prep notebook'\n",
        "FOLDER = 'FOLDER from data prep notebook'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "EVAL_BASE = 'eval_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'cultivated'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "EVAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "BUFFER_SIZE = 3000\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'binary_crossentropy'\n",
        "METRICS = ['binary_accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiv9GGXUozyB"
      },
      "source": [
        "Let's check if the config file has been written correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUzgnsdGo9cc"
      },
      "source": [
        "!cat {PACKAGE_PATH}/config.py\n",
        "\n",
        "from cdl_ai_platform import config\n",
        "print('\\n\\n', config.BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR29bQR5pIX-"
      },
      "source": [
        "Now let's write a model file which first creates a training and evaluation datasets from the data prepared in `DataPrep.ipynb` and then defines the model that the training job needs to train.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vrsc5U2ptWz"
      },
      "source": [
        "%%writefile {PACKAGE_PATH}/model.py\n",
        "\n",
        "from . import config\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import optimizers\n",
        "\n",
        "# Dataset loading functions\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "  return tf.io.parse_single_example(example_proto, config.FEATURES_DICT)\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  inputsList = [inputs.get(key) for key in config.FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(config.BANDS)], stacked[:,:,len(config.BANDS):]\n",
        "\n",
        "def get_dataset(pattern):\n",
        "\tglob = tf.io.gfile.glob(pattern)\n",
        "\tdataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "\tdataset = dataset.map(parse_tfrecord)\n",
        "\tdataset = dataset.map(to_tuple)\n",
        "\treturn dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "\tglob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.TRAINING_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.shuffle(config.BUFFER_SIZE).batch(config.BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "def get_eval_dataset():\n",
        "  glob = 'gs://' + config.DATA_BUCKET + '/' + config.FOLDER + '/' + config.EVAL_BASE + '*'\n",
        "  dataset = get_dataset(glob)\n",
        "  dataset = dataset.batch(1).repeat()\n",
        "  return dataset\n",
        "\n",
        "# Model Definition\n",
        "\n",
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(config.BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(config.NCLASS, (1,1), activation='softmax')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(config.OPTIMIZER), \n",
        "\t\tloss=losses.get(config.LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in config.METRICS])\n",
        "\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NapUL78Cp-YI"
      },
      "source": [
        "Now let's do the sanity check on both dataset creation and the model definition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPrPy8_yp9_y"
      },
      "source": [
        "from cdl_ai_platform import model\n",
        "\n",
        "eval = model.get_eval_dataset()\n",
        "print(iter(eval.take(1)).next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CR4TgSsqSRD"
      },
      "source": [
        "model = model.get_model()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnxFEV1XqajF"
      },
      "source": [
        "Now let's define the task that the AI Platform Training job will run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-NwrR0xqnwt"
      },
      "source": [
        "%%writefile {PACKAGE_PATH}/task.py\n",
        "\n",
        "from . import config\n",
        "from . import model\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  training = model.get_training_dataset()\n",
        "  evaluation = model.get_eval_dataset()\n",
        "\n",
        "  m = model.get_model()\n",
        "\n",
        "  m.fit(\n",
        "      x=training,\n",
        "      epochs=config.EPOCHS, \n",
        "      steps_per_epoch=int(config.TRAIN_SIZE / config.BATCH_SIZE), \n",
        "      validation_data=evaluation,\n",
        "      validation_steps=int(config.EVAL_SIZE),\n",
        "      callbacks=[tf.keras.callbacks.TensorBoard(config.LOGS_DIR)])\n",
        "\n",
        "  m.save(config.MODEL_DIR, save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HHPmQl3qwnB"
      },
      "source": [
        "# Training Job Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-PtuGdnEGcv"
      },
      "source": [
        "JOB_NAME = 'unique_job_name'\n",
        "TRAINER_PACKAGE_PATH = 'cdl_ai_platform'\n",
        "MAIN_TRAINER_MODULE = 'cdl_ai_platform.task'\n",
        "REGION = 'select your region'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ylNNelX7hmP"
      },
      "source": [
        "!gcloud config set project {config.PROJECT}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zt7VreXrimO"
      },
      "source": [
        "The following command will create the job on GCP AI platform and your `PROJECT` will be billed for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPKOZ9_hr6eO"
      },
      "source": [
        "!gcloud ai-platform jobs submit training {JOB_NAME} \\\n",
        "    --job-dir {config.JOB_DIR}  \\\n",
        "    --package-path {TRAINER_PACKAGE_PATH} \\\n",
        "    --module-name {MAIN_TRAINER_MODULE} \\\n",
        "    --region {REGION} \\\n",
        "    --project {config.PROJECT} \\\n",
        "    --runtime-version 2.1 \\\n",
        "    --python-version 3.7 \\\n",
        "    --scale-tier basic-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYAKhgYDsM51"
      },
      "source": [
        "# Training Job Monitoring\n",
        "The training job can be monitored directly from the GCP AI Platform Jobs Console or you can visualize it in the notebook using ***tensorboard***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlfdVQs_sxNC"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {config.LOGS_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
